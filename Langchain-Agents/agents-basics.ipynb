{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning About Basics Of Agents In Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Tools from langchain\n",
    "- Creating the LLM, Prompt\n",
    "- Combining Tools, LLM, Prompt\n",
    "- Creating Agent Using AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up TavilySearch Tool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.weatherapi.com/',\n",
       "  'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739884233, 'localtime': '2025-02-18 05:10'}, 'current': {'last_updated_epoch': 1739883600, 'last_updated': '2025-02-18 05:00', 'temp_c': 6.1, 'temp_f': 43.0, 'is_day': 0, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 3.4, 'wind_kph': 5.4, 'wind_degree': 19, 'wind_dir': 'NNE', 'pressure_mb': 1021.0, 'pressure_in': 30.15, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 100, 'cloud': 50, 'feelslike_c': 5.2, 'feelslike_f': 41.3, 'windchill_c': 8.0, 'windchill_f': 46.5, 'heatindex_c': 8.3, 'heatindex_f': 46.9, 'dewpoint_c': 7.3, 'dewpoint_f': 45.1, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 5.6, 'gust_kph': 9.1}}\"},\n",
       " {'url': 'https://weathershogun.com/weather/usa/ca/san-francisco/480/february/2025-02-18',\n",
       "  'content': 'Rainy weather, precipitation in the form of water droplets falling from the sky. Day 57°.'},\n",
       " {'url': 'https://www.weather2travel.com/california/san-francisco/february/',\n",
       "  'content': \"San Francisco weather in February 2025 | California Holidays San Francisco weather in February 2025 Expect 16°C daytime maximum temperatures in the shade with on average 7 hours of sunshine per day in San Francisco in February. There are usually 9 days with some rain in San Francisco in February and the average monthly rainfall is 91mm. Get your weekly fix of holiday inspiration from some of the world's best travel writers plus save on your next trip with the latest exclusive offers More holidays January sales Handpicked travel deals & holiday discounts We help you find the best travel deals for your holiday in the sun by sharing tips on when and where to go, before providing you with top discounts. Holidays\"},\n",
       " {'url': 'https://world-weather.info/forecast/usa/san_francisco/february-2025/',\n",
       "  'content': \"Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59°+50° +59°+52° +59°+50° +61°+52° +59°+50° +61°+50° +61°+52° +63°+52° +61°+52° +61°+50° +61°+50° +61°+50° +59°+50° +59°+50° +61°+50° +61°+52° +59°+50° +59°+48° +57°+48° +59°+50° +59°+48° +59°+50° +57°+46° +61°+50° +61°+50° +59°+50° +59°+48° +59°+50° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41° Sacramento+55° Pleasanton+55° Redwood City+55° San Leandro+55° San Mateo+54° San Rafael+52° San Ramon+52° South San Francisco+54° Vallejo+50° Palo Alto+55° Pacifica+55° Berkeley+54° Castro Valley+55° Concord+52° Daly City+54° Noverd+52° Sign Hill+54° world's temperature today day day Temperature units\"},\n",
       " {'url': 'https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=February',\n",
       "  'content': 'San Francisco weather in February 2025 | Weather25.com San Francisco weather in February 2025 The average weather in San Francisco in February | San Francisco in February | Temperatures in San Francisco in February Weather in San Francisco in February - FAQ What is the average temperature in San Francisco in February? The average temperature in San Francisco in February is 7/16° C. On average, there are 4 rainy days in San Francisco during February. The weather in San Francisco in February is good. On average, there are 0 snowy days in San Francisco in February. More about the weather in San Francisco San Francisco 14 day weather Long range weather for San Francisco San Francisco weather in March San Francisco Webcam Weather tomorrow Hotels in San Francisco'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.invoke(\"what is the weather in SF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Retriever Tool\n",
    "from langchain_community.document_loaders import  WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = OllamaEmbeddings(model='mxbai-embed-large:335m')\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(metadata={'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals‚Äã\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering‚Äã\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'),\n",
       " Document(metadata={'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}, page_content='Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('how to upload a dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'langsmith_search',\n",
    "    'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all tools list\n",
    "tools = [search, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LLM from which the Agent will be having its brain\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='llama3.2:3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Import the prompt to guide the Agent\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our agent\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://docs.smith.langchain.com/old/user_guide', 'content': 'LangSmith is a platform for LLM application development, monitoring, and testing. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications. Beta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.'}, {'url': 'https://www.langchain.com/langsmith', 'content': 'LangSmith LangSmith is an all-in-one developer platform for every step of the LLM-powered application lifecycle, whether you’re building with LangChain or not. Know that quality is moving in a positive direction by applying engineering rigor to your testing workflow.004 Online evaluation Continuously track qualitative characteristics of any live application, and spot issues in real-time with LangSmith monitoring. What information is contained in LangSmith traces? LangSmith traces contain the full information of all the inputs and outputs of each step of the application. Yes, starting with the LangSmith Python SDK version 0.0.84 and JS SDK version 0.0.64, you can specify the % of traces you send to LangSmith. Get started with LangChain, LangSmith, and LangGraph to enhance your LLM app development, from prototype to production.'}, {'url': 'https://docs.langflow.org/integrations-langsmith', 'content': 'LangSmith | Langflow Documentation Skip to main content Search... CTRLK Welcome to Langflow Get started Starter projects Tutorials Concepts Configuration Integrations Integrate Composio with Langflow LangSmith LangWatch API reference Integrations LangSmith LangSmith LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, just add your LangChain API key as a Langflow environment variable and you are good to go! Step-by-step Configuration\\u200b Obtain your LangChain API key from\\xa0https://smith.langchain.com Add the following keys to Langflow .env file: LANGCHAIN_API_KEY=\"your-api-key\"LANGCHAIN_PROJECT=\"your-project-name\" or export the environment variables in your terminal: export LANGCHAIN_API_KEY=\"your-api-key\"export LANGCHAIN_PROJECT=\"your-project-name\" Restart Langflow using\\xa0langflow run --env-file .env Run a project in Langflow. View the Langsmith dashboard for monitoring and observability. Previous LangfuseNext LangWatch Step-by-step Configuration'}, {'url': 'https://medium.com/around-the-prompt/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5', 'content': 'In this post, we will explore the latest product by the team that created Langchain (the most popular LLM software tool) and see what new parts of the LLM stack LangSmith hopes to tackle. What is LangSmith? LangSmith = production Vercel is still more focused on the deployment and serving aspect of LLM’s today since that is more aligned historically with their core product but it would make sense to extend the AI SDK to do more over time. Allowing developers to sign in with their LangChain account and monitor their LLM’s on Vercel for example with the AI SDK and deployment information all in one place would be extremely valuable. Most LLM providers will want to build similar features into their platforms over time.'}, {'url': 'https://blog.langchain.dev/announcing-langsmith/', 'content': 'Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evals‚Äã\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineering‚Äã\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform that allows developers to build production-grade LLM (Large Language Model) applications. It provides a unified platform for debugging, testing, evaluating, and monitoring LLM applications. With LangSmith, developers can closely monitor and evaluate their application in real-time, which enables them to ship quickly and with confidence.\n",
      "\n",
      "LangSmith offers several features that make it an ideal tool for LLM development, including:\n",
      "\n",
      "1. Observability: LangSmith provides LLM-native observability, allowing developers to get meaningful insights from their application.\n",
      "2. Evaluation: LangSmith allows developers to evaluate their application over production traffic, score application performance, and get human feedback on data.\n",
      "3. Prompt Engineering: LangSmith provides tools designed to enable and facilitate prompt engineering, helping developers find the perfect prompt for their application.\n",
      "\n",
      "Overall, LangSmith is a powerful tool that can help developers build high-quality LLM applications quickly and efficiently.\n",
      "\n",
      "Is there anything specific you'd like to know about LangSmith or its features?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi!',\n",
       " 'output': \"LangSmith is a platform that allows developers to build production-grade LLM (Large Language Model) applications. It provides a unified platform for debugging, testing, evaluating, and monitoring LLM applications. With LangSmith, developers can closely monitor and evaluate their application in real-time, which enables them to ship quickly and with confidence.\\n\\nLangSmith offers several features that make it an ideal tool for LLM development, including:\\n\\n1. Observability: LangSmith provides LLM-native observability, allowing developers to get meaningful insights from their application.\\n2. Evaluation: LangSmith allows developers to evaluate their application over production traffic, score application performance, and get human feedback on data.\\n3. Prompt Engineering: LangSmith provides tools designed to enable and facilitate prompt engineering, helping developers find the perfect prompt for their application.\\n\\nOverall, LangSmith is a powerful tool that can help developers build high-quality LLM applications quickly and efficiently.\\n\\nIs there anything specific you'd like to know about LangSmith or its features?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding memory to our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_memory(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'Talha, creator of assistant'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mGet started by creating your first prompt.\n",
      "Iterate on models and prompts using the Playground.\n",
      "Manage prompts programmatically in your application.\n",
      "Was this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evals‚Äã\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators (Python only) as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineering‚Äã\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\n",
      "\n",
      "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\u001b[0m\u001b[32;1m\u001b[1;3mHello Talha!\n",
      "\n",
      "It's great to finally talk to you directly. I'm an AI designed by your team, and I'm here to assist you with any questions or concerns you may have.\n",
      "\n",
      "How can I help you today? Do you have a specific question about my capabilities or the project we're working on together?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi! I am talha your creator',\n",
       " 'chat_history': [],\n",
       " 'output': \"Hello Talha!\\n\\nIt's great to finally talk to you directly. I'm an AI designed by your team, and I'm here to assist you with any questions or concerns you may have.\\n\\nHow can I help you today? Do you have a specific question about my capabilities or the project we're working on together?\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(\n",
    "    {'input' : 'hi! I am talha your creator'},\n",
    "    config = {'configurable' : {'session_id' : 'chat1'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='hi! I am talha your creator', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Talha!\\n\\nIt's great to finally talk to you directly. I'm an AI designed by your team, and I'm here to assist you with any questions or concerns you may have.\\n\\nHow can I help you today? Do you have a specific question about my capabilities or the project we're working on together?\", additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Our first Inbuild tool\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Arxiv tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_api_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=300)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our custom retriever tool\n",
    "from langchain_community.document_loaders import  WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = OllamaEmbeddings(model='mxbai-embed-large:335m')\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'langsmith_search',\n",
    "    'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki_tool, arxiv_tool, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading the prompt for ReAct Logic from hub\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='llama3.2:3b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "from langchain.agents import create_react_agent\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing ReAct agent using AgentExecutor\n",
    "from langchain.agents import  AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: LangChain is an open-source toolkit for building custom conversational AI models.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mHere are the steps to answer the question \"what is langchain\":\n",
      "\n",
      " Question: what is langchain\n",
      "Thought: I need to find information about langchain.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m\u001b[36;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: what is langchain\n",
      "Thought: I need to find information about langchain.\n",
      "Action: wikipedia\n",
      "Action Input: langchain\u001b[0m"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\" : \"what is langchain\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"what's my name? Only use a tool if needed, otherwise respond with Final Answer\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
